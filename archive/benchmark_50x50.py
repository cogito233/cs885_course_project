"""
å®Œæ•´æ€§èƒ½æµ‹è¯•
- GPU 2
- Qwen3-14B-Base
- 50æ¡è½¨è¿¹ Ã— 50è½®å¯¹è¯
- Batch Size 128
- å¼ºåˆ¶generateæ¯è½®assistantå›å¤
"""

import sglang as sgl
import os
import time
import json
import yaml


os.environ["CUDA_VISIBLE_DEVICES"] = "2"

MODEL_PATH = "/data/minimax-dialogue/users/ruobai/cogito/base_model/Qwen3-14B-Base"
JSONL_FILE = "/data/minimax-dialogue/users/ruobai/cogito_dev/course_project_854/20250826_070310_MULTI_xiancai-80_swe_verified_train_0_5000.jsonl"
YAML_FILE = "/data/minimax-dialogue/users/ruobai/r2e-gym-xiancai/src/r2egym/agenthub/config/r2egym/edit_non_fn_calling_.yaml"


def load_config():
    with open(YAML_FILE, 'r') as f:
        return yaml.safe_load(f)


def load_trajectories(num=50):
    trajs = []
    with open(JSONL_FILE, 'r') as f:
        for i, line in enumerate(f):
            if i >= num:
                break
            trajs.append(json.loads(line))
    return trajs


@sgl.function
def multiturn_gen(s, user_msgs, num_turns):
    """å¤šè½®å¯¹è¯ç”Ÿæˆ"""
    for i in range(num_turns):
        if i < len(user_msgs):
            s += sgl.user(user_msgs[i])
        s += sgl.assistant(sgl.gen(f"turn_{i}", max_tokens=128, temperature=0.7))


def benchmark(runtime, trajs, batch_size, config, num_turns):
    """æ€§èƒ½æµ‹è¯•"""
    print(f"\n{'='*70}")
    print(f"æµ‹è¯•: {num_turns}è½® Ã— {len(trajs)}æ¡ Ã— Batch Size {batch_size}")
    print(f"{'='*70}")
    
    system_prompt = config['system_prompt']
    instance_template = config['instance_prompt']
    
    # å‡†å¤‡æ•°æ®
    all_data = []
    total_env_time = 0
    
    for traj in trajs:
        problem = traj.get('problem_statement', '')[:500]
        first_msg = f"Problem: {problem}"
        
        user_msgs = [first_msg]
        traj_env_time = 0
        
        steps = traj.get('trajectory_steps', [])[:num_turns]
        for step in steps:
            env_time = float(step.get('env_exec_time', 0))
            traj_env_time += env_time
            obs = step.get('observation', '')[:200]
            user_msgs.append(f"{obs}...\n[Env: {env_time:.3f}s]")
        
        all_data.append({
            "user_msgs": user_msgs,
            "env_time": traj_env_time
        })
        total_env_time += traj_env_time
    
    avg_env = total_env_time / len(all_data)
    print(f"å‡†å¤‡å®Œæˆ, å¹³å‡envæ—¶é—´: {avg_env:.2f}ç§’/è½¨è¿¹")
    
    # å¼€å§‹å¤„ç†
    print(f"å¼€å§‹å¤„ç†...")
    start_time = time.time()
    processed = 0
    total_tokens = 0
    
    for i in range(0, len(all_data), batch_size):
        batch = all_data[i:i+batch_size]
        
        for data in batch:
            try:
                state = multiturn_gen.run(
                    user_msgs=data["user_msgs"],
                    num_turns=num_turns
                )
                processed += 1
                
                # ç»Ÿè®¡tokens
                for j in range(num_turns):
                    try:
                        resp = state[f"turn_{j}"]
                        if resp:
                            total_tokens += len(resp.split())
                    except:
                        pass
                
                # è¿›åº¦
                if processed % 10 == 0:
                    elapsed = time.time() - start_time
                    print(f"  {processed}/{len(all_data)} "
                          f"({100*processed/len(all_data):.0f}%) "
                          f"{processed/elapsed:.1f}æ¡/ç§’ "
                          f"{elapsed:.1f}ç§’")
                        
            except Exception as e:
                if processed == 0:
                    print(f"  é”™è¯¯: {str(e)[:150]}")
                continue
    
    total_time = time.time() - start_time
    
    result = {
        "num_turns": num_turns,
        "batch_size": batch_size,
        "num_trajectories": len(trajs),
        "processed": processed,
        "total_time": total_time,
        "throughput": processed/total_time,
        "avg_time_per_traj": total_time/max(processed,1),
        "total_tokens": total_tokens,
        "avg_tokens_per_traj": total_tokens/max(processed,1),
        "avg_tokens_per_turn": total_tokens/max(processed,1)/num_turns,
        "avg_env_time": avg_env,
        "total_env_time": total_env_time
    }
    
    print(f"\nç»“æœ:")
    print(f"  æ€»è€—æ—¶: {total_time:.2f}ç§’")
    print(f"  ååé‡: {result['throughput']:.2f}æ¡/ç§’")
    print(f"  æ¯æ¡è€—æ—¶: {result['avg_time_per_traj']:.2f}ç§’")
    print(f"  æ€»tokens: {total_tokens}")
    print(f"  å¹³å‡: {result['avg_tokens_per_traj']:.0f} tokens/è½¨è¿¹")
    print(f"  å¹³å‡: {result['avg_tokens_per_turn']:.1f} tokens/è½®")
    print(f"  Envæ—¶é—´: {avg_env:.2f}ç§’/è½¨è¿¹")
    print(f"{'='*70}")
    
    return result


def main():
    print("=" * 70)
    print("å®Œæ•´æ€§èƒ½æµ‹è¯•: 50è½¨è¿¹ Ã— 50è½® Ã— BS128")
    print("GPU 2 | Qwen3-14B-Base")
    print("=" * 70)
    print(f"æ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S')}\n")
    
    # åŠ è½½
    print("[1/3] åŠ è½½...")
    config = load_config()
    trajs = load_trajectories(50)
    print(f"âœ“ {len(trajs)}æ¡è½¨è¿¹\n")
    
    # åŠ è½½æ¨¡å‹
    print("[2/3] åŠ è½½æ¨¡å‹...")
    start = time.time()
    runtime = sgl.Runtime(
        model_path=MODEL_PATH,
        tp_size=1,
        mem_fraction_static=0.8,
        max_total_tokens=8192,
    )
    sgl.set_default_backend(runtime)
    load_time = time.time() - start
    print(f"âœ“ å®Œæˆ! {load_time:.1f}ç§’")
    
    print("\nGPU 2æ˜¾å­˜:")
    os.system("nvidia-smi --query-gpu=index,memory.used --format=csv,noheader | grep '^2,'")
    
    # æµ‹è¯•
    print(f"\n[3/3] æ€§èƒ½æµ‹è¯•...")
    
    all_results = []
    
    # æ¸è¿›æµ‹è¯•: 5è½® -> 10è½® -> 20è½® -> 50è½®
    for num_turns in [5, 10, 20, 50]:
        try:
            result = benchmark(runtime, trajs, 128, config, num_turns)
            all_results.append(result)
            time.sleep(1)
        except Exception as e:
            print(f"\nâœ— {num_turns}è½®å¤±è´¥: {str(e)[:150]}")
            break
    
    # æ±‡æ€»
    print(f"\n\n{'='*70}")
    print("å®Œæ•´æµ‹è¯•æ±‡æ€»")
    print("="*70)
    print(f"{'è½®æ•°':<8} {'æ€»è€—æ—¶(ç§’)':<14} {'ååé‡':<12} {'æ€»Tokens':<12} {'å¹³å‡Tok/è½®':<12} {'Envæ—¶é—´(ç§’)':<12}")
    print("-"*70)
    for r in all_results:
        print(f"{r['num_turns']:<8} "
              f"{r['total_time']:<14.2f} "
              f"{r['throughput']:<12.2f} "
              f"{r['total_tokens']:<12} "
              f"{r['avg_tokens_per_turn']:<12.1f} "
              f"{r['avg_env_time']:<12.2f}")
    print("="*70)
    
    # é‡ç‚¹å±•ç¤º50è½®ç»“æœ
    if all_results and all_results[-1]['num_turns'] == 50:
        r = all_results[-1]
        print(f"\nğŸ¯ 50è½®å¯¹è¯å…³é”®æŒ‡æ ‡:")
        print(f"  å®Œæˆ50æ¡Ã—50è½®: {r['total_time']:.2f}ç§’")
        print(f"  ååé‡: {r['throughput']:.2f}æ¡/ç§’")
        print(f"  æ¯æ¡è½¨è¿¹: {r['avg_time_per_traj']:.2f}ç§’")
        print(f"  æ€»ç”Ÿæˆ: {r['total_tokens']} tokens")
        print(f"  å¹³å‡: {r['avg_tokens_per_traj']:.0f} tokens/è½¨è¿¹")
        print(f"  å¹³å‡: {r['avg_tokens_per_turn']:.1f} tokens/è½®")
        print(f"  Envæ—¶é—´: {r['avg_env_time']:.2f}ç§’/è½¨è¿¹")
    
    # ä¿å­˜
    with open("benchmark_50x50_bs128_gpu2_qwen3.json", 'w') as f:
        json.dump({
            "model": "Qwen3-14B-Base",
            "gpu": "GPU 2",
            "batch_size": 128,
            "num_trajectories": 50,
            "model_load_time": load_time,
            "timestamp": time.strftime('%Y-%m-%d %H:%M:%S'),
            "results": all_results
        }, f, indent=2)
    print(f"\nç»“æœå·²ä¿å­˜: benchmark_50x50_bs128_gpu2_qwen3.json")
    
    # å…³é—­
    runtime.shutdown()
    print("\nâœ“ æ‰€æœ‰æµ‹è¯•å®Œæˆ!")


if __name__ == "__main__":
    main()

