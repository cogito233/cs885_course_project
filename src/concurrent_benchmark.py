"""
å¹¶å‘åŸºå‡†æµ‹è¯• - æ­£ç¡®çš„batchå®ç°
batch_size = åŒæ—¶è¿è¡Œçš„è½¨è¿¹æ•°é‡ï¼Œå®Œæˆä¸€æ¡å°±è¡¥å……ä¸€æ¡

GPU 2, Qwen3-14B-Base
50æ¡è½¨è¿¹ Ã— 5è½®å¯¹è¯
æµ‹è¯•batch_size: 1, 2, 4, 8, 16, 32, 64, 128, 256
"""

import sglang as sgl
import os, time, json, yaml, sys, argparse
from concurrent.futures import ThreadPoolExecutor, as_completed

MODEL_PATH = "/data/minimax-dialogue/users/ruobai/cogito/base_model/Qwen3-14B-Base"
JSONL_FILE = "/data/minimax-dialogue/users/ruobai/cogito_dev/course_project_854/20250826_070310_MULTI_xiancai-80_swe_verified_train_0_5000.jsonl"
YAML_FILE = "/data/minimax-dialogue/users/ruobai/r2e-gym-xiancai/src/r2egym/agenthub/config/r2egym/edit_non_fn_calling_.yaml"

def log(msg):
    print(f"[{time.strftime('%H:%M:%S')}] {msg}")
    sys.stdout.flush()

def load_config():
    with open(YAML_FILE) as f:
        return yaml.safe_load(f)

def load_trajectories(num=50):
    trajs = []
    with open(JSONL_FILE) as f:
        for i, line in enumerate(f):
            if i >= num:
                break
            trajs.append(json.loads(line))
    return trajs

@sgl.function
def multiturn_generate(s, system_msg, first_user_msg, observations, num_turns):
    """å¤šè½®å¯¹è¯ç”Ÿæˆ"""
    s += sgl.system(system_msg)
    s += sgl.user(first_user_msg)
    
    for i in range(num_turns):
        # å¼ºåˆ¶generate
        s += sgl.assistant(sgl.gen(
            f"turn_{i}",
            max_tokens=256,
            temperature=0.7,
            stop=["<|im_end|>", "\n\nUSER:"]
        ))
        
        # ä¸‹ä¸€ä¸ªuseræ¶ˆæ¯
        if i < len(observations):
            s += sgl.user(observations[i])

def prepare_trajectory_data(traj, config, num_turns):
    """å‡†å¤‡å•æ¡è½¨è¿¹æ•°æ® - å®Œæ•´å†…å®¹"""
    system_prompt = config['system_prompt']
    instance_template = config['instance_prompt']
    
    # å®Œæ•´çš„problem_statement
    problem_statement = traj.get('problem_statement', '')
    first_user_msg = instance_template.format(problem_statement=problem_statement)
    
    # å®Œæ•´çš„observations
    steps = traj.get('trajectory_steps', [])[:num_turns]
    observations = []
    total_env_time = 0
    
    for step in steps:
        env_time = float(step.get('env_exec_time', 0))
        total_env_time += env_time
        
        # å®Œæ•´çš„observation
        observation = step.get('observation', '')
        obs_msg = f"{observation}\n\n[Environment execution time: {env_time:.4f}s]"
        observations.append(obs_msg)
    
    return system_prompt, first_user_msg, observations, total_env_time

def process_single_trajectory(traj_id, data, num_turns):
    """å¤„ç†å•æ¡è½¨è¿¹"""
    try:
        start = time.time()
        state = multiturn_generate.run(
            system_msg=data["system_msg"],
            first_user_msg=data["first_user_msg"],
            observations=data["observations"],
            num_turns=num_turns
        )
        elapsed = time.time() - start
        
        # ç»Ÿè®¡tokens
        total_tokens = 0
        for i in range(num_turns):
            try:
                resp = state[f"turn_{i}"]
                if resp:
                    total_tokens += len(resp.split())
            except:
                pass
        
        return {
            "success": True,
            "traj_id": traj_id,
            "time": elapsed,
            "tokens": total_tokens
        }
    except Exception as e:
        return {
            "success": False,
            "traj_id": traj_id,
            "error": str(e)[:100]
        }

def benchmark_concurrent(runtime, trajs, batch_size, config, num_turns):
    """
    å¹¶å‘æµ‹è¯• - batch_size = åŒæ—¶è¿è¡Œçš„è½¨è¿¹æ•°é‡
    ä½¿ç”¨çº¿ç¨‹æ± å®ç°ï¼šåŒæ—¶è¿è¡Œbatch_sizeä¸ªä»»åŠ¡ï¼Œå®Œæˆä¸€ä¸ªå°±è¡¥å……ä¸€ä¸ª
    """
    log(f"\n{'='*70}")
    log(f"æµ‹è¯• Batch Size (å¹¶å‘æ•°): {batch_size}")
    log(f"{'='*70}")
    
    # å‡†å¤‡æ‰€æœ‰è½¨è¿¹æ•°æ®
    log("å‡†å¤‡æ•°æ®...")
    all_data = []
    total_env_time = 0
    
    for idx, traj in enumerate(trajs):
        system_msg, first_user_msg, observations, env_time = prepare_trajectory_data(
            traj, config, num_turns
        )
        all_data.append({
            "system_msg": system_msg,
            "first_user_msg": first_user_msg,
            "observations": observations,
            "env_time": env_time
        })
        total_env_time += env_time
        
        if (idx+1) % 10 == 0:
            log(f"  å‡†å¤‡: {idx+1}/{len(trajs)}")
    
    avg_env_time = total_env_time / len(all_data)
    log(f"âœ“ æ•°æ®å‡†å¤‡å®Œæˆ")
    log(f"  å¹³å‡envæ—¶é—´: {avg_env_time:.2f}ç§’/è½¨è¿¹")
    
    # ä½¿ç”¨çº¿ç¨‹æ± å¹¶å‘å¤„ç†
    log(f"å¼€å§‹å¹¶å‘æ¨ç†ï¼ˆå¹¶å‘æ•°={batch_size}ï¼‰...")
    start_time = time.time()
    
    processed = 0
    failed = 0
    total_tokens = 0
    
    with ThreadPoolExecutor(max_workers=batch_size) as executor:
        # æäº¤æ‰€æœ‰ä»»åŠ¡
        futures = {
            executor.submit(process_single_trajectory, idx, data, num_turns): idx
            for idx, data in enumerate(all_data)
        }
        
        # æŒ‰å®Œæˆé¡ºåºå¤„ç†ç»“æœ
        for future in as_completed(futures):
            result = future.result()
            
            if result["success"]:
                processed += 1
                total_tokens += result["tokens"]
                
                # æ¯5æ¡è¾“å‡ºä¸€æ¬¡è¿›åº¦
                if processed % 5 == 0:
                    elapsed = time.time() - start_time
                    log(f"  è¿›åº¦: {processed}/{len(all_data)} "
                        f"({100*processed/len(all_data):.0f}%) "
                        f"é€Ÿåº¦: {processed/elapsed:.2f}æ¡/ç§’ "
                        f"ç”¨æ—¶: {elapsed:.1f}ç§’ "
                        f"è½¨è¿¹#{result['traj_id']}:{result['tokens']}tok/{result['time']:.2f}ç§’")
            else:
                failed += 1
                if failed <= 2:
                    log(f"  å¤±è´¥ #{failed}: {result['error']}")
    
    total_time = time.time() - start_time
    
    result_summary = {
        "batch_size": batch_size,
        "num_turns": num_turns,
        "num_trajectories": len(trajs),
        "processed": processed,
        "failed": failed,
        "total_time": total_time,
        "throughput": processed/total_time if total_time > 0 else 0,
        "avg_time_per_traj": total_time/max(processed,1),
        "total_tokens": total_tokens,
        "avg_tokens_per_traj": total_tokens/max(processed,1),
        "avg_tokens_per_turn": total_tokens/max(processed,1)/num_turns if num_turns > 0 else 0,
        "avg_env_time": avg_env_time,
        "total_env_time": total_env_time
    }
    
    log(f"\nç»“æœ:")
    log(f"  æˆåŠŸ: {processed}/{len(trajs)}")
    log(f"  æ€»è€—æ—¶: {total_time:.2f}ç§’")
    log(f"  ååé‡: {result_summary['throughput']:.2f}æ¡/ç§’")
    log(f"  æ¯æ¡è€—æ—¶: {result_summary['avg_time_per_traj']:.2f}ç§’")
    log(f"  æ€»tokens: {total_tokens}")
    log(f"  å¹³å‡: {result_summary['avg_tokens_per_traj']:.0f} tokens/è½¨è¿¹")
    log(f"  å¹³å‡: {result_summary['avg_tokens_per_turn']:.1f} tokens/è½®")
    log(f"  Envæ—¶é—´: {avg_env_time:.2f}ç§’/è½¨è¿¹")
    log("="*70)
    
    return result_summary

def main():
    if __name__ != '__main__':
        return
    
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    parser = argparse.ArgumentParser(description='å¹¶å‘åŸºå‡†æµ‹è¯•')
    parser.add_argument('--gpu', type=int, default=2, help='ä½¿ç”¨çš„GPUç¼–å· (é»˜è®¤: 2)')
    parser.add_argument('--turns', type=int, default=5, help='å¯¹è¯è½®æ•° (é»˜è®¤: 5)')
    parser.add_argument('--num_traj', type=int, default=50, help='è½¨è¿¹æ•°é‡ (é»˜è®¤: 50)')
    args = parser.parse_args()
    
    # è®¾ç½®GPU
    os.environ["CUDA_VISIBLE_DEVICES"] = str(args.gpu)
    
    log("="*70)
    log("å¹¶å‘åŸºå‡†æµ‹è¯• - æµ‹è¯•ä¸åŒBatch Sizeçš„å½±å“")
    log("Batch Size = åŒæ—¶è¿è¡Œçš„è½¨è¿¹æ•°é‡ï¼ˆå¹¶å‘æ•°ï¼‰")
    log("="*70)
    log(f"é…ç½®: GPU {args.gpu}, {args.num_traj}æ¡è½¨è¿¹, {args.turns}è½®å¯¹è¯")
    log("="*70)
    
    # åŠ è½½
    log("[1/3] åŠ è½½é…ç½®å’Œæ•°æ®...")
    config = load_config()
    trajs = load_trajectories(args.num_traj)
    log(f"âœ“ åŠ è½½äº† {len(trajs)} æ¡è½¨è¿¹")
    log(f"  æ¯æ¡çº¦ {len(trajs[0].get('trajectory_steps', []))} æ­¥")
    
    # åŠ è½½æ¨¡å‹
    log(f"\n[2/3] åŠ è½½æ¨¡å‹åˆ°GPU {args.gpu}...")
    start = time.time()
    runtime = sgl.Runtime(
        model_path=MODEL_PATH,
        tp_size=1,
        mem_fraction_static=0.8,
        max_total_tokens=65536  # 64kä¸Šä¸‹æ–‡çª—å£
    )
    sgl.set_default_backend(runtime)
    load_time = time.time() - start
    log(f"âœ“ æ¨¡å‹åŠ è½½å®Œæˆ! {load_time:.1f}ç§’")
    
    log(f"\nGPU {args.gpu}æ˜¾å­˜:")
    os.system(f"nvidia-smi --query-gpu=index,memory.used --format=csv,noheader | grep '^{args.gpu},'")
    
    # æµ‹è¯•ä¸åŒbatch size
    log("\n[3/3] æµ‹è¯•ä¸åŒBatch Size (å¹¶å‘æ•°)...")
    log("æµ‹è¯•: 1, 2, 4, 8, 16, 32, 64, 128, 256")
    
    num_turns = args.turns
    batch_sizes = [1, 2, 4, 8, 16, 32, 64, 128, 256]
    all_results = []
    
    for batch_size in batch_sizes:
        try:
            result = benchmark_concurrent(runtime, trajs, batch_size, config, num_turns)
            all_results.append(result)
            time.sleep(2)
        except Exception as e:
            log(f"âœ— Batch Size {batch_size} å¤±è´¥: {str(e)[:150]}")
            import traceback
            traceback.print_exc()
            continue
    
    # æ±‡æ€»
    log("\n\n" + "="*70)
    log("æµ‹è¯•æ±‡æ€» - ä¸åŒBatch Size (å¹¶å‘æ•°) çš„å½±å“")
    log("="*70)
    print(f"{'å¹¶å‘æ•°':<8} {'æ€»è€—æ—¶(ç§’)':<14} {'ååé‡':<14} {'æ¯æ¡(ç§’)':<12} {'æ€»Tokens':<12} {'Tok/è½®':<10}")
    print("-"*70)
    for r in all_results:
        print(f"{r['batch_size']:<8} "
              f"{r['total_time']:<14.2f} "
              f"{r['throughput']:<14.2f} "
              f"{r['avg_time_per_traj']:<12.2f} "
              f"{r['total_tokens']:<12} "
              f"{r['avg_tokens_per_turn']:<10.1f}")
    log("="*70)
    
    # æ‰¾å‡ºæœ€ä¼˜
    if all_results:
        best = max(all_results, key=lambda x: x['throughput'])
        log(f"\nğŸ¯ æœ€ä¼˜å¹¶å‘æ•°: {best['batch_size']}")
        log(f"  ååé‡: {best['throughput']:.2f}æ¡/ç§’")
        log(f"  å®Œæˆ50æ¡éœ€è¦: {best['total_time']:.2f}ç§’")
        log(f"  ç›¸æ¯”å¹¶å‘æ•°=1: æå‡{(best['throughput']/all_results[0]['throughput']-1)*100:.1f}%")
    
    # ä¿å­˜
    output_file = f"concurrent_batch_comparison_gpu{args.gpu}_{args.num_traj}traj_{num_turns}turns.json"
    with open(output_file, 'w') as f:
        json.dump({
            "model": "Qwen3-14B-Base",
            "gpu": f"GPU {args.gpu}",
            "num_trajectories": args.num_traj,
            "num_turns": num_turns,
            "model_load_time": load_time,
            "timestamp": time.strftime('%Y-%m-%d %H:%M:%S'),
            "note": "Concurrent processing: batch_size = number of trajectories running simultaneously",
            "results": all_results
        }, f, indent=2)
    log(f"\nç»“æœå·²ä¿å­˜: {output_file}")
    
    runtime.shutdown()
    log("\nâœ“ æ‰€æœ‰æµ‹è¯•å®Œæˆ!")

if __name__ == '__main__':
    main()

