"""
Stateful KV Cache ç‰ˆæœ¬ - Runtime ç»´æŠ¤ KV state
- SGLang ä¸€æ¬¡æ€§å®šä¹‰å¤šè½®å¯¹è¯æµç¨‹ï¼ŒRuntime è‡ªåŠ¨å¤ç”¨ KVï¼ˆæ ¸å¿ƒä¼˜åŒ–ï¼‰
- é¿å…æ¯è½® full prefillï¼Œåª decode æ–° tokenï¼ˆèŠ‚çœ >99% prefillï¼‰
- æ»‘åŠ¨çª—å£è®¡ç®—ååé‡ï¼ˆ10ç§’çª—å£ï¼Œå¸¦è¡°å‡ï¼‰
- é¢„è®¡æ€§èƒ½æå‡ï¼š3-4x token ååé‡
- GPU 1/2/3, Qwen2.5-14B (128k)
- 50æ¡ Ã— 50è½® Ã— BS=[3,4,6,8,12,16]
"""

import sglang as sgl
import os, time, json, yaml, sys, argparse
from concurrent.futures import ThreadPoolExecutor, as_completed
from threading import Lock
from collections import defaultdict, deque

MODEL_PATH = "/data/minimax-dialogue/users/ruobai/cogito/base_model/Qwen2.5-14B"
JSONL_FILE = "/data/minimax-dialogue/users/ruobai/cogito_dev/course_project_854/20250826_070310_MULTI_xiancai-80_swe_verified_train_0_5000.jsonl"
YAML_FILE = "/data/minimax-dialogue/users/ruobai/r2e-gym-xiancai/src/r2egym/agenthub/config/r2egym/edit_non_fn_calling_.yaml"

def log(msg):
    print(f"[{time.strftime('%H:%M:%S')}] {msg}")
    sys.stdout.flush()

# å…¨å±€çŠ¶æ€
class GlobalState:
    def __init__(self):
        self.lock = Lock()
        self.active_trajs = set()  # æ­£åœ¨è¿è¡Œçš„è½¨è¿¹ID
        self.completed_trajs = 0
        self.total_tokens = 0
        # è®°å½•æ¯ä¸ªæ´»è·ƒè½¨è¿¹çš„å½“å‰å‰ç¼€é•¿åº¦
        self.traj_prefix_lengths = defaultdict(int)
        self.start_time = None
        # æ»‘åŠ¨çª—å£ï¼šè®°å½•æœ€è¿‘çš„ token ç”Ÿæˆ (timestamp, tokens)
        self.recent_tokens = deque(maxlen=100)  # æœ€è¿‘100æ¬¡ç”Ÿæˆ
        self.recent_window_seconds = 10.0  # æ»‘åŠ¨çª—å£ 10 ç§’

global_state = GlobalState()

def load_config():
    with open(YAML_FILE) as f:
        return yaml.safe_load(f)

def load_trajectories(num=50):
    trajs = []
    with open(JSONL_FILE) as f:
        for i, line in enumerate(f):
            if i >= num:
                break
            trajs.append(json.loads(line))
    return trajs

@sgl.function
def rollout_trajectory_stateful(s, system_msg, first_user_msg, observations, num_turns):
    """
    Stateful å¤šè½®å¯¹è¯ï¼šä¸€æ¬¡æ€§å®šä¹‰æ•´ä¸ªæµç¨‹ï¼ŒRuntime è‡ªåŠ¨å¤ç”¨ KV
    - ä¸å†æ¯è½® full prefill
    - SGLang å†…éƒ¨ç»´æŠ¤ KV cache state
    """
    # åˆå§‹ä¸Šä¸‹æ–‡ï¼ˆåª prefill ä¸€æ¬¡ï¼‰
    s += sgl.system(system_msg)
    s += sgl.user(first_user_msg)
    
    # å¤šè½®å¯¹è¯ï¼šæ¯è½®åª append æ–°å†…å®¹
    for turn_idx in range(num_turns):
        # ç”Ÿæˆ assistant å›å¤
        s += sgl.assistant(sgl.gen(
            f"response_{turn_idx}",
            max_tokens=256,
            temperature=0.7,
            stop=["<|im_end|>", "\n\nUSER:"]
        ))
        
        # æ·»åŠ ä¸‹ä¸€è½®çš„ user observationï¼ˆå¦‚æœæœ‰ï¼‰
        if turn_idx < len(observations):
            s += sgl.user(observations[turn_idx])

def prepare_trajectory_data(traj, config, num_turns):
    """å‡†å¤‡è½¨è¿¹æ•°æ®"""
    system_prompt = config['system_prompt']
    instance_template = config['instance_prompt']
    
    problem_statement = traj.get('problem_statement', '')
    first_user_msg = instance_template.format(problem_statement=problem_statement)
    
    steps = traj.get('trajectory_steps', [])[:num_turns]
    observations = []
    total_env_time = 0
    
    for step in steps:
        env_time = float(step.get('env_exec_time', 0))
        total_env_time += env_time
        observation = step.get('observation', '')
        obs_msg = f"{observation}\n\n[Environment execution time: {env_time:.4f}s]"
        observations.append(obs_msg)
    
    return system_prompt, first_user_msg, observations, total_env_time

def calculate_sliding_window_throughput():
    """è®¡ç®—æ»‘åŠ¨çª—å£å†…çš„ token ååé‡ï¼ˆå¸¦æ—¶é—´è¡°å‡ï¼‰"""
    if not global_state.recent_tokens:
        return 0.0
    
    current_time = time.time()
    cutoff_time = current_time - global_state.recent_window_seconds
    
    # åªç»Ÿè®¡çª—å£å†…çš„ tokens
    tokens_in_window = []
    for timestamp, tokens in global_state.recent_tokens:
        if timestamp >= cutoff_time:
            tokens_in_window.append((timestamp, tokens))
    
    if not tokens_in_window:
        return 0.0
    
    total_tokens = sum(t[1] for t in tokens_in_window)
    time_span = current_time - tokens_in_window[0][0]
    
    if time_span <= 0:
        return 0.0
    
    return total_tokens / time_span

def process_trajectory_per_turn(traj_id, data, num_turns, batch_size, metadata_file):
    """
    Stateful å¤„ç†ï¼šä¸€æ¬¡è°ƒç”¨å®Œæˆæ‰€æœ‰è½®æ¬¡ï¼ŒRuntime å†…éƒ¨å¤ç”¨ KV
    æ¯å®Œæˆä¸€è½®è®°å½•ä¸€æ¬¡
    """
    try:
        # æ ‡è®°å¼€å§‹
        with global_state.lock:
            global_state.active_trajs.add(traj_id)
        
        # ä¼°ç®—åˆå§‹å‰ç¼€é•¿åº¦
        initial_prefix = len(data["system_msg"].split()) + len(data["first_user_msg"].split())
        
        with global_state.lock:
            global_state.traj_prefix_lengths[traj_id] = initial_prefix
        
        # ä¸€æ¬¡æ€§ç”Ÿæˆæ‰€æœ‰è½®æ¬¡ï¼ˆRuntime å†…éƒ¨ç»´æŠ¤ KV stateï¼‰
        overall_start = time.time()
        
        state = rollout_trajectory_stateful.run(
            system_msg=data["system_msg"],
            first_user_msg=data["first_user_msg"],
            observations=data["observations"],
            num_turns=num_turns
        )
        
        overall_elapsed = time.time() - overall_start
        
        # é€è½®æå–ç»“æœå¹¶è®°å½•
        for turn_idx in range(num_turns):
            response_key = f"response_{turn_idx}"
            response = state.get(response_key, "") if hasattr(state, 'get') else state[response_key]
            turn_tokens = len(response.split()) if response else 0
            
            # ä¼°ç®—è¿™ä¸€è½®çš„æ—¶é—´ï¼ˆå¹³å‡åˆ†é…ï¼Œå®é™…ä¸Š Runtime æ˜¯å¹¶è¡Œçš„ï¼‰
            turn_time = overall_elapsed / num_turns
            
            # æ›´æ–°å…¨å±€çŠ¶æ€
            with global_state.lock:
                global_state.total_tokens += turn_tokens
                global_state.traj_prefix_lengths[traj_id] += turn_tokens
                
                # è®°å½•åˆ°æ»‘åŠ¨çª—å£
                current_time = time.time()
                global_state.recent_tokens.append((current_time, turn_tokens))
                
                # è®¡ç®—æ»‘åŠ¨çª—å£ååé‡
                sliding_throughput = calculate_sliding_window_throughput()
                
                # è®¡ç®—ç´¯è®¡ååé‡ï¼ˆç”¨äºå¯¹æ¯”ï¼‰
                total_prefix = sum(global_state.traj_prefix_lengths.values())
                elapsed_since_start = time.time() - global_state.start_time
                cumulative_throughput = global_state.total_tokens / elapsed_since_start if elapsed_since_start > 0 else 0
                
                # è®°å½•è¿™ä¸€è½®çš„æŒ‡æ ‡
                record = {
                    "batch_size": batch_size,
                    "timestamp": elapsed_since_start,
                    "traj_id": traj_id,
                    "turn_idx": turn_idx,
                    "turn_tokens": turn_tokens,
                    "turn_time": turn_time,
                    "completed_trajs": global_state.completed_trajs,
                    "active_trajs": len(global_state.active_trajs),
                    "total_tokens_generated": global_state.total_tokens,
                    "prefix_length_sum": total_prefix,
                    "token_throughput_cumulative": cumulative_throughput,  # ç´¯è®¡å¹³å‡
                    "token_throughput_sliding": sliding_throughput  # æ»‘åŠ¨çª—å£ï¼ˆæ›´å‡†ç¡®ï¼‰
                }
            
            # å†™å…¥æ–‡ä»¶
            with open(metadata_file, 'a') as f:
                f.write(json.dumps(record) + '\n')
                f.flush()
            
            # æ›´æ–°å‰ç¼€é•¿åº¦ï¼ˆåŠ ä¸Š observationï¼‰
            if turn_idx < len(data["observations"]):
                obs_tokens = len(data["observations"][turn_idx].split())
                with global_state.lock:
                    global_state.traj_prefix_lengths[traj_id] += obs_tokens
        
        # è½¨è¿¹å®Œæˆ
        with global_state.lock:
            global_state.active_trajs.remove(traj_id)
            global_state.completed_trajs += 1
            del global_state.traj_prefix_lengths[traj_id]
        
        return {"success": True, "traj_id": traj_id, "overall_time": overall_elapsed}
        
    except Exception as e:
        with global_state.lock:
            if traj_id in global_state.active_trajs:
                global_state.active_trajs.remove(traj_id)
            if traj_id in global_state.traj_prefix_lengths:
                del global_state.traj_prefix_lengths[traj_id]
        return {"success": False, "traj_id": traj_id, "error": str(e)[:200]}

def benchmark_per_turn(runtime, trajs, batch_size, config, num_turns, metadata_file):
    """å¹¶å‘æµ‹è¯•ï¼Œæ¯è½®è®°å½•ä¸€æ¬¡"""
    log(f"\n{'='*70}")
    log(f"æµ‹è¯• Batch Size (å¹¶å‘æ•°): {batch_size}")
    log(f"{'='*70}")
    
    # å‡†å¤‡æ•°æ®
    log("å‡†å¤‡æ•°æ®...")
    all_data = []
    total_env_time = 0
    
    for idx, traj in enumerate(trajs):
        system_msg, first_user_msg, observations, env_time = prepare_trajectory_data(
            traj, config, num_turns
        )
        all_data.append({
            "system_msg": system_msg,
            "first_user_msg": first_user_msg,
            "observations": observations,
            "env_time": env_time
        })
        total_env_time += env_time
        
        if (idx+1) % 10 == 0:
            log(f"  å‡†å¤‡: {idx+1}/{len(trajs)}")
    
    avg_env_time = total_env_time / len(all_data)
    log(f"âœ“ å‡†å¤‡å®Œæˆ, avg env: {avg_env_time:.2f}ç§’")
    
    # é‡ç½®å…¨å±€çŠ¶æ€
    global global_state
    global_state = GlobalState()
    global_state.start_time = time.time()
    
    # å†™å…¥batch_sizeæ ‡è®°
    with open(metadata_file, 'a') as f:
        f.write(f"# Batch Size {batch_size} starts\n")
        f.flush()
    
    log(f"å¼€å§‹å¹¶å‘æ¨ç†ï¼ˆå¹¶å‘æ•°={batch_size}ï¼‰...")
    log("ğŸ’¡ Stateful æ¨¡å¼ï¼šæ¯æ¡è½¨è¿¹ä¸€æ¬¡æ€§ç”Ÿæˆæ‰€æœ‰è½®æ¬¡")
    log("   â†’ Runtime å†…éƒ¨ç»´æŠ¤ KV cacheï¼Œé¿å…é‡å¤ prefillï¼ˆæœ€å…³é”®ä¼˜åŒ–ï¼‰")
    log("   â†’ é¦–è½® prefill å®Œæ•´ä¸Šä¸‹æ–‡ï¼Œåç»­è½®æ¬¡åª decode æ–° token")
    log("   â†’ SGLang è‡ªåŠ¨ dynamic batching å¤„ç†å¹¶å‘è¯·æ±‚")
    
    start_time = time.time()
    processed = 0
    failed = 0
    
    # å¹¶å‘å¤„ç†
    with ThreadPoolExecutor(max_workers=batch_size) as executor:
        futures = {
            executor.submit(process_trajectory_per_turn, idx, data, num_turns, batch_size, metadata_file): idx
            for idx, data in enumerate(all_data)
        }
        
        for future in as_completed(futures):
            result = future.result()
            
            if result["success"]:
                processed += 1
                
                if processed % 5 == 0:
                    elapsed = time.time() - start_time
                    with global_state.lock:
                        metrics = {
                            "completed": global_state.completed_trajs,
                            "active": len(global_state.active_trajs),
                            "tokens": global_state.total_tokens
                        }
                        sliding_tp = calculate_sliding_window_throughput()
                    
                    # è®¡ç®—å®æ—¶ååé‡
                    traj_throughput = processed / elapsed if elapsed > 0 else 0
                    token_throughput_avg = metrics['tokens'] / elapsed if elapsed > 0 else 0
                    avg_time_per_traj = elapsed / processed if processed > 0 else 0
                    
                    log(f"  è¿›åº¦: {processed}/{len(all_data)} "
                        f"({100*processed/len(all_data):.0f}%) | "
                        f"è½¨è¿¹: {traj_throughput:.3f}æ¡/ç§’ | "
                        f"Token(æ»‘çª—): {sliding_tp:.2f}tok/ç§’ | "
                        f"Token(ç´¯è®¡): {token_throughput_avg:.2f}tok/ç§’ | "
                        f"å·²ç”¨æ—¶: {elapsed:.1f}ç§’")
            else:
                failed += 1
                if failed <= 3:
                    log(f"  å¤±è´¥: {result['error']}")
    
    total_time = time.time() - start_time
    
    result_summary = {
        "batch_size": batch_size,
        "num_turns": num_turns,
        "processed": processed,
        "failed": failed,
        "total_time": total_time,
        "avg_time_per_traj": total_time/max(processed, 1),
        "throughput_traj_per_sec": processed/total_time if total_time > 0 else 0,
        "throughput_token_per_sec": global_state.total_tokens/total_time if total_time > 0 else 0,
        "total_tokens": global_state.total_tokens,
        "avg_tokens_per_traj": global_state.total_tokens/max(processed, 1),
        "avg_tokens_per_turn": global_state.total_tokens/max(processed, 1)/num_turns if num_turns > 0 else 0,
        "avg_env_time": avg_env_time,
        "records_per_traj": num_turns  # æ¯æ¡è½¨è¿¹è®°å½•num_turnsæ¬¡
    }
    
    log(f"\n{'='*70}")
    log(f"Batch Size {batch_size} æµ‹è¯•å®Œæˆ")
    log(f"{'='*70}")
    log(f"  âœ“ æˆåŠŸ: {processed}/{len(trajs)} æ¡è½¨è¿¹")
    log(f"  âœ— å¤±è´¥: {failed} æ¡")
    log(f"  â±ï¸  æ€»è€—æ—¶: {total_time:.2f} ç§’ (å¹³å‡ {result_summary['avg_time_per_traj']:.2f}ç§’/æ¡)")
    log(f"  ğŸ“Š è½¨è¿¹åå: {result_summary['throughput_traj_per_sec']:.3f} æ¡/ç§’")
    log(f"  ğŸš€ Tokenåå: {result_summary['throughput_token_per_sec']:.2f} tok/ç§’")
    log(f"  ğŸ’¬ æ€»tokens: {result_summary['total_tokens']:,}")
    log(f"  ğŸ“ å¹³å‡tokens: {result_summary['avg_tokens_per_traj']:.0f} tokens/è½¨è¿¹")
    log(f"  ğŸ“ˆ è®°å½•æ•°: {processed * num_turns:,} æ¡ï¼ˆæ¯è½¨è¿¹{num_turns}æ¡ï¼‰")
    log("="*70)
    
    return result_summary

def main():
    if __name__ != '__main__':
        return
    
    parser = argparse.ArgumentParser()
    parser.add_argument('--gpu', type=int, default=1)
    parser.add_argument('--turns', type=int, default=50)
    parser.add_argument('--num_traj', type=int, default=50)
    parser.add_argument('--batch_sizes', type=str, default='2,3,4,6,8', 
                        help='é€—å·åˆ†éš”çš„batch sizeåˆ—è¡¨ï¼Œå¦‚ "3,4" æˆ– "6,8"')
    args = parser.parse_args()
    
    # è§£æbatch_sizes
    batch_sizes = [int(x.strip()) for x in args.batch_sizes.split(',')]
    
    os.environ["CUDA_VISIBLE_DEVICES"] = str(args.gpu)
    
    log("="*70)
    log("ğŸ”¬ Stateful KV Cache ç‰ˆæœ¬ - Runtime ç»´æŠ¤ KV State")
    log("="*70)
    log("ğŸš€ æ ¸å¿ƒä¼˜åŒ–: é¿å…æ¯è½® full prefillï¼ŒRuntime è‡ªåŠ¨å¤ç”¨ KV")
    log("   â†’ èŠ‚çœ >99% çš„ prefill è®¡ç®—ï¼Œé¢„è®¡ååæå‡ 3-4x")
    log("ğŸ“Š è¿½è¸ª: æ»‘åŠ¨çª—å£ååé‡ï¼ˆ10ç§’çª—å£ï¼Œå¸¦è¡°å‡ï¼‰")
    log("="*70)
    log(f"ğŸ¤– æ¨¡å‹: Qwen2.5-14B (128kä¸Šä¸‹æ–‡)")
    log(f"ğŸ¯ é…ç½®: GPU {args.gpu} | {args.num_traj}æ¡è½¨è¿¹ | {args.turns}è½®å¯¹è¯")
    log(f"ğŸ”¢ æµ‹è¯•: Batch Size = {batch_sizes}")
    log("="*70)
    
    # åŠ è½½
    log("\n[1/3] åŠ è½½...")
    config = load_config()
    trajs = load_trajectories(args.num_traj)
    log(f"âœ“ {len(trajs)}æ¡è½¨è¿¹")
    
    # åŠ è½½æ¨¡å‹
    log(f"\n[2/3] åŠ è½½æ¨¡å‹...")
    log("ä¼˜åŒ–é…ç½®: æé«˜æ˜¾å­˜åˆ©ç”¨ç‡")
    start = time.time()
    
    # æ ¹æ®GPUç¼–å·è®¾ç½®ä¸åŒçš„ç«¯å£ï¼Œé¿å…å†²çª
    port = 30000 + args.gpu * 10
    log(f"ä½¿ç”¨ç«¯å£: {port}")
    
    # GPU ç«¯ä¼˜åŒ–å‚æ•°ï¼ˆæ ¹æ® SGLang ç‰ˆæœ¬è°ƒæ•´ï¼‰
    runtime_kwargs = {
        "model_path": MODEL_PATH,
        "tp_size": 1,
        "mem_fraction_static": 0.88,   # æé«˜åˆ°88%ä»¥å……åˆ†åˆ©ç”¨æ˜¾å­˜
        "max_total_tokens": 131072,    # 128k tokens KV cache
        "port": port                   # æ¯ä¸ªGPUä½¿ç”¨ä¸åŒç«¯å£
    }
    
    # å°è¯•æ·»åŠ é«˜çº§ä¼˜åŒ–å‚æ•°ï¼ˆå¦‚æœ SGLang ç‰ˆæœ¬æ”¯æŒï¼‰
    # æ³¨é‡Šæ‰çš„å‚æ•°å¯èƒ½åœ¨æ—§ç‰ˆæœ¬ä¸­ä¸æ”¯æŒ
    # runtime_kwargs["schedule_policy"] = "fcfs"
    # runtime_kwargs["chunked_prefill_size"] = 8192
    # runtime_kwargs["enable_mixed_chunk"] = True
    # runtime_kwargs["schedule_conservativeness"] = 1.0
    
    runtime = sgl.Runtime(**runtime_kwargs)
    sgl.set_default_backend(runtime)
    load_time = time.time() - start
    log(f"âœ“ {load_time:.1f}ç§’")
    
    log(f"\nGPU {args.gpu}æ˜¾å­˜ä½¿ç”¨:")
    os.system(f"nvidia-smi --query-gpu=index,memory.used,memory.total --format=csv,noheader,nounits | grep '^{args.gpu},'")
    
    log("\nğŸ’¡ æ ¸å¿ƒä¼˜åŒ–è¯´æ˜:")
    log("  ã€Stateful KV Cache - æœ€å…³é”®ä¼˜åŒ–ã€‘")
    log("    â€¢ ä¸€æ¬¡æ€§å®šä¹‰æ•´ä¸ªå¤šè½®å¯¹è¯æµç¨‹")
    log("    â€¢ Runtime å†…éƒ¨ç»´æŠ¤ KV stateï¼Œé¿å…æ¯è½® full prefill")
    log("    â€¢ é¦–è½® prefill + åç»­åª decodeï¼Œå¤§å¹…é™ä½è®¡ç®—å¼€é”€ï¼ˆèŠ‚çœ >99% prefillï¼‰")
    log("  ã€æ˜¾å­˜ç®¡ç†ã€‘")
    log("    â€¢ mem_fraction=0.88: å……åˆ†åˆ©ç”¨æ˜¾å­˜ç©ºé—´ï¼ˆvs é»˜è®¤0.8ï¼‰")
    log("    â€¢ max_total_tokens=131k: æ›´å¤§çš„KV cacheå®¹é‡ï¼ˆæ”¯æŒé•¿å¯¹è¯ï¼‰")
    log("  ã€ååé‡è®¡ç®—ã€‘")
    log("    â€¢ æ»‘åŠ¨çª—å£ï¼ˆ10ç§’ï¼‰ï¼šåæ˜ å½“å‰å®æ—¶æ€§èƒ½")
    log("    â€¢ ç´¯è®¡å¹³å‡ï¼šåæ˜ æ•´ä½“å¹³å‡æ€§èƒ½")
    log("  ã€æ³¨æ„ã€‘")
    log("    â€¢ éƒ¨åˆ†é«˜çº§è°ƒåº¦å‚æ•°å·²æ³¨é‡Šï¼ˆSGLang ç‰ˆæœ¬å…¼å®¹æ€§ï¼‰")
    log("    â€¢ Stateful KV Cache æ˜¯æœ€æ ¸å¿ƒä¼˜åŒ–ï¼Œæ•ˆæœæœ€æ˜¾è‘—")
    
    # æµ‹è¯•
    log(f"\n[3/3] æµ‹è¯•...")
    
    # æ–‡ä»¶ååŒ…å«batch_sizesä¿¡æ¯
    bs_str = '_'.join(map(str, batch_sizes))
    metadata_file = f"per_turn_metrics_stateful_gpu{args.gpu}_{args.num_traj}traj_{args.turns}turns_bs{bs_str}.jsonl"
    
    # æ¸…ç©ºæ–‡ä»¶
    with open(metadata_file, 'w') as f:
        f.write(f"# Stateful KV Cache metrics: GPU{args.gpu}, {args.num_traj} trajs, {args.turns} turns, BS={batch_sizes}\n")
        f.write(f"# Runtime maintains KV state, avoids per-turn full prefill\n")
        f.write(f"# token_throughput_sliding: 10s sliding window with decay\n")
        f.write(f"# token_throughput_cumulative: overall average\n")
    
    all_results = []
    
    for batch_size in batch_sizes:
        try:
            log(f"\n{'>'*70}")
            log(f"Batch Size {batch_size}")
            result = benchmark_per_turn(
                runtime, trajs, batch_size, config, args.turns, metadata_file
            )
            all_results.append(result)
            time.sleep(2)
        except Exception as e:
            log(f"âœ— BS {batch_size} å¤±è´¥: {str(e)[:150]}")
            import traceback
            traceback.print_exc()
            continue
    
    # æ±‡æ€»
    log("\n" + "="*80)
    log("ğŸ¯ æµ‹è¯•æ±‡æ€» - Stateful KV Cache æ‰€æœ‰Batch Sizeç»“æœå¯¹æ¯”")
    log("="*80)
    print(f"{'BS':<4} {'æ€»è€—æ—¶':<10} {'å¹³å‡/æ¡':<10} {'è½¨è¿¹åå':<14} {'Tokenåå':<14} {'æ€»Tokens':<12} {'Tok/è½®':<10}")
    print(f"{'':4} {'(ç§’)':<10} {'(ç§’)':<10} {'(æ¡/ç§’)':<14} {'(tok/ç§’)':<14} {'':<12} {'':<10}")
    print("-"*80)
    for r in all_results:
        print(f"{r['batch_size']:<4} "
              f"{r['total_time']:<10.2f} "
              f"{r['avg_time_per_traj']:<10.2f} "
              f"{r['throughput_traj_per_sec']:<14.3f} "
              f"{r['throughput_token_per_sec']:<14.2f} "
              f"{r['total_tokens']:<12,} "
              f"{r['avg_tokens_per_turn']:<10.1f}")
    log("="*80)
    log("ğŸ’¡ æ³¨æ„ï¼šä½¿ç”¨ Stateful KV Cache åï¼Œååé‡åº”æ˜¾è‘—æå‡")
    log("   å¯¹æ¯”åŸç‰ˆå¯çœ‹å‡ºé¿å…é‡å¤ prefill çš„æ”¶ç›Š")
    
    # æ‰¾å‡ºæœ€ä¼˜é…ç½®
    if all_results:
        best_traj = max(all_results, key=lambda x: x['throughput_traj_per_sec'])
        best_token = max(all_results, key=lambda x: x['throughput_token_per_sec'])
        fastest = min(all_results, key=lambda x: x['avg_time_per_traj'])
        
        log(f"\nğŸ† æ€§èƒ½æœ€ä¼˜:")
        log(f"  â€¢ è½¨è¿¹ååæœ€é«˜: BS={best_traj['batch_size']}, {best_traj['throughput_traj_per_sec']:.3f}æ¡/ç§’")
        log(f"  â€¢ Tokenååæœ€é«˜: BS={best_token['batch_size']}, {best_token['throughput_token_per_sec']:.2f}tok/ç§’")
        log(f"  â€¢ å•æ¡æœ€å¿«: BS={fastest['batch_size']}, {fastest['avg_time_per_traj']:.2f}ç§’/æ¡")
    log("="*80)
    
    # ä¿å­˜
    summary_file = f"per_turn_summary_stateful_gpu{args.gpu}_{args.num_traj}traj_{args.turns}turns_bs{bs_str}.json"
    with open(summary_file, 'w') as f:
        json.dump({
            "model": "Qwen2.5-14B",
            "gpu": args.gpu,
            "num_trajectories": args.num_traj,
            "num_turns": args.turns,
            "model_load_time": load_time,
            "timestamp": time.strftime('%Y-%m-%d %H:%M:%S'),
            "optimization": "Stateful KV Cache - Runtime maintains KV state, avoids per-turn full prefill",
            "note": "Each trajectory generates all turns in one Runtime call, KV cache automatically reused",
            "throughput_metric": "sliding_window (10s) reflects real-time performance",
            "metadata_file": metadata_file,
            "results": all_results
        }, f, indent=2)
    
    log(f"\nâœ“ Metadata: {metadata_file}")
    log(f"âœ“ Summary: {summary_file}")
    log(f"æ€»è®°å½•æ•°: {sum(r['processed'] * r['records_per_traj'] for r in all_results)}")
    
    runtime.shutdown()
    log("\nâœ“ å®Œæˆ!")

if __name__ == '__main__':
    main()

