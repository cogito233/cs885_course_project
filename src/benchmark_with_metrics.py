"""
å¸¦è¯¦ç»†æŒ‡æ ‡è¿½è¸ªçš„åŸºå‡†æµ‹è¯•
- æ¨¡å‹: Qwen2.5-14B (128kä¸Šä¸‹æ–‡)
- GPU 1
- 50æ¡è½¨è¿¹ Ã— 50è½®å¯¹è¯
- Batch Size: 2, 3, 4, 6, 8
- è¿½è¸ª: æ—¶é—´ vs tokenååé‡, æ—¶é—´ vs å‰ç¼€é•¿åº¦
- ä¿å­˜metadataåˆ°jsonlä¾›åç»­ç”»å›¾
"""

import sglang as sgl
import os, time, json, yaml, sys, argparse
from concurrent.futures import ThreadPoolExecutor, as_completed
from threading import Lock

MODEL_PATH = "/data/minimax-dialogue/users/ruobai/cogito/base_model/Qwen2.5-14B"
JSONL_FILE = "/data/minimax-dialogue/users/ruobai/cogito_dev/course_project_854/20250826_070310_MULTI_xiancai-80_swe_verified_train_0_5000.jsonl"
YAML_FILE = "/data/minimax-dialogue/users/ruobai/r2e-gym-xiancai/src/r2egym/agenthub/config/r2egym/edit_non_fn_calling_.yaml"

def log(msg):
    print(f"[{time.strftime('%H:%M:%S')}] {msg}")
    sys.stdout.flush()

def load_config():
    with open(YAML_FILE) as f:
        return yaml.safe_load(f)

def load_trajectories(num=50):
    trajs = []
    with open(JSONL_FILE) as f:
        for i, line in enumerate(f):
            if i >= num:
                break
            trajs.append(json.loads(line))
    return trajs

@sgl.function
def multiturn_generate(s, system_msg, first_user_msg, observations, num_turns):
    """å¤šè½®å¯¹è¯ç”Ÿæˆ"""
    s += sgl.system(system_msg)
    s += sgl.user(first_user_msg)
    
    for i in range(num_turns):
        s += sgl.assistant(sgl.gen(
            f"turn_{i}",
            max_tokens=256,
            temperature=0.7,
            stop=["<|im_end|>", "\n\nUSER:"]
        ))
        
        if i < len(observations):
            s += sgl.user(observations[i])

def prepare_trajectory_data(traj, config, num_turns):
    """å‡†å¤‡å•æ¡è½¨è¿¹æ•°æ® - å®Œæ•´å†…å®¹"""
    system_prompt = config['system_prompt']
    instance_template = config['instance_prompt']
    
    # å®Œæ•´çš„problem_statement
    problem_statement = traj.get('problem_statement', '')
    first_user_msg = instance_template.format(problem_statement=problem_statement)
    
    # å®Œæ•´çš„observations
    steps = traj.get('trajectory_steps', [])[:num_turns]
    observations = []
    total_env_time = 0
    
    for step in steps:
        env_time = float(step.get('env_exec_time', 0))
        total_env_time += env_time
        
        # å®Œæ•´çš„observation
        observation = step.get('observation', '')
        obs_msg = f"{observation}\n\n[Environment execution time: {env_time:.4f}s]"
        observations.append(obs_msg)
    
    return system_prompt, first_user_msg, observations, total_env_time

def process_single_trajectory(traj_id, data, num_turns):
    """å¤„ç†å•æ¡è½¨è¿¹ï¼Œè¿”å›è¯¦ç»†æŒ‡æ ‡"""
    try:
        start_time = time.time()
        state = multiturn_generate.run(
            system_msg=data["system_msg"],
            first_user_msg=data["first_user_msg"],
            observations=data["observations"],
            num_turns=num_turns
        )
        elapsed = time.time() - start_time
        
        # ç»Ÿè®¡ç”Ÿæˆçš„tokens
        total_tokens = 0
        turn_tokens = []
        for i in range(num_turns):
            try:
                resp = state[f"turn_{i}"]
                if resp:
                    tokens = len(resp.split())
                    total_tokens += tokens
                    turn_tokens.append(tokens)
                else:
                    turn_tokens.append(0)
            except:
                turn_tokens.append(0)
        
        return {
            "success": True,
            "traj_id": traj_id,
            "time": elapsed,
            "tokens": total_tokens,
            "turn_tokens": turn_tokens,
            "env_time": data["env_time"]
        }
    except Exception as e:
        return {
            "success": False,
            "traj_id": traj_id,
            "error": str(e)[:200]
        }

def benchmark_concurrent_with_metrics(runtime, trajs, batch_size, config, num_turns, metadata_file):
    """
    å¹¶å‘æµ‹è¯•ï¼Œè®°å½•è¯¦ç»†çš„æ—¶é—´å’ŒtokenæŒ‡æ ‡
    """
    log(f"\n{'='*70}")
    log(f"æµ‹è¯• Batch Size (å¹¶å‘æ•°): {batch_size}")
    log(f"{'='*70}")
    
    # å‡†å¤‡æ‰€æœ‰è½¨è¿¹æ•°æ®
    log("å‡†å¤‡æ•°æ®...")
    all_data = []
    total_env_time = 0
    
    for idx, traj in enumerate(trajs):
        system_msg, first_user_msg, observations, env_time = prepare_trajectory_data(
            traj, config, num_turns
        )
        all_data.append({
            "system_msg": system_msg,
            "first_user_msg": first_user_msg,
            "observations": observations,
            "env_time": env_time
        })
        total_env_time += env_time
        
        if (idx+1) % 10 == 0:
            log(f"  å‡†å¤‡: {idx+1}/{len(trajs)}")
    
    avg_env_time = total_env_time / len(all_data)
    log(f"âœ“ æ•°æ®å‡†å¤‡å®Œæˆ")
    log(f"  å¹³å‡envæ—¶é—´: {avg_env_time:.2f}ç§’/è½¨è¿¹")
    
    # å¼€å§‹å¹¶å‘å¤„ç†ï¼Œè®°å½•è¯¦ç»†æŒ‡æ ‡
    log(f"å¼€å§‹å¹¶å‘æ¨ç†ï¼ˆå¹¶å‘æ•°={batch_size}ï¼‰...")
    start_time = time.time()
    
    processed = 0
    failed = 0
    total_tokens = 0
    
    # ç”¨äºè¿½è¸ªå®æ—¶æŒ‡æ ‡
    metrics_lock = Lock()
    time_series = []  # æ—¶é—´åºåˆ—æ•°æ®
    
    # æ‰“å¼€metadataæ–‡ä»¶å‡†å¤‡å†™å…¥
    with open(metadata_file, 'a') as f_meta:
        with ThreadPoolExecutor(max_workers=batch_size) as executor:
            # æäº¤æ‰€æœ‰ä»»åŠ¡
            futures = {
                executor.submit(process_single_trajectory, idx, data, num_turns): idx
                for idx, data in enumerate(all_data)
            }
            
            # æŒ‰å®Œæˆé¡ºåºå¤„ç†ç»“æœ
            for future in as_completed(futures):
                result = future.result()
                current_time = time.time()
                elapsed = current_time - start_time
                
                if result["success"]:
                    processed += 1
                    traj_tokens = result["tokens"]
                    total_tokens += traj_tokens
                    
                    # è®¡ç®—å½“å‰æŒ‡æ ‡
                    current_throughput = processed / elapsed if elapsed > 0 else 0
                    token_throughput = total_tokens / elapsed if elapsed > 0 else 0
                    
                    # è®¡ç®—å‰ç¼€é•¿åº¦ï¼ˆè¿‘ä¼¼ï¼šå·²å®Œæˆè½¨è¿¹çš„ç´¯ç§¯tokenæ•°ï¼‰
                    prefix_length = total_tokens
                    
                    # è®°å½•æ—¶é—´åºåˆ—æ•°æ®
                    with metrics_lock:
                        time_point = {
                            "batch_size": batch_size,
                            "timestamp": elapsed,
                            "completed_trajs": processed,
                            "total_tokens_generated": total_tokens,
                            "trajectory_throughput": current_throughput,
                            "token_throughput": token_throughput,
                            "prefix_length_approx": prefix_length,
                            "traj_id": result["traj_id"],
                            "traj_time": result["time"],
                            "traj_tokens": traj_tokens,
                            "traj_env_time": result["env_time"]
                        }
                        time_series.append(time_point)
                        
                        # å®æ—¶å†™å…¥metadata
                        f_meta.write(json.dumps(time_point) + '\n')
                        f_meta.flush()
                    
                    # è¾“å‡ºè¿›åº¦
                    if processed % 10 == 0:
                        log(f"  è¿›åº¦: {processed}/{len(all_data)} "
                            f"({100*processed/len(all_data):.0f}%) "
                            f"é€Ÿåº¦: {current_throughput:.2f}æ¡/ç§’ "
                            f"Tokenåå: {token_throughput:.2f}tok/ç§’ "
                            f"ç”¨æ—¶: {elapsed:.1f}ç§’")
                else:
                    failed += 1
                    if failed <= 3:
                        log(f"  å¤±è´¥ #{failed}: {result['error']}")
    
    total_time = time.time() - start_time
    
    result_summary = {
        "batch_size": batch_size,
        "num_turns": num_turns,
        "num_trajectories": len(trajs),
        "processed": processed,
        "failed": failed,
        "total_time": total_time,
        "throughput_traj_per_sec": processed/total_time if total_time > 0 else 0,
        "throughput_token_per_sec": total_tokens/total_time if total_time > 0 else 0,
        "avg_time_per_traj": total_time/max(processed,1),
        "total_tokens": total_tokens,
        "avg_tokens_per_traj": total_tokens/max(processed,1),
        "avg_tokens_per_turn": total_tokens/max(processed,1)/num_turns if num_turns > 0 else 0,
        "avg_env_time": avg_env_time,
        "total_env_time": total_env_time,
        "time_series_points": len(time_series)
    }
    
    log(f"\nç»“æœ:")
    log(f"  æˆåŠŸ: {processed}/{len(trajs)}")
    log(f"  æ€»è€—æ—¶: {total_time:.2f}ç§’")
    log(f"  è½¨è¿¹åå: {result_summary['throughput_traj_per_sec']:.2f}æ¡/ç§’")
    log(f"  Tokenåå: {result_summary['throughput_token_per_sec']:.2f}tok/ç§’")
    log(f"  æ¯æ¡è€—æ—¶: {result_summary['avg_time_per_traj']:.2f}ç§’")
    log(f"  æ€»tokens: {total_tokens}")
    log(f"  å¹³å‡: {result_summary['avg_tokens_per_traj']:.0f} tokens/è½¨è¿¹")
    log(f"  å¹³å‡: {result_summary['avg_tokens_per_turn']:.1f} tokens/è½®")
    log(f"  Envæ—¶é—´: {avg_env_time:.2f}ç§’/è½¨è¿¹")
    log(f"  æ—¶é—´åºåˆ—ç‚¹æ•°: {len(time_series)}")
    log("="*70)
    
    return result_summary

def main():
    if __name__ != '__main__':
        return
    
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    parser = argparse.ArgumentParser(description='å¸¦æŒ‡æ ‡è¿½è¸ªçš„å¹¶å‘åŸºå‡†æµ‹è¯•')
    parser.add_argument('--gpu', type=int, default=1, help='ä½¿ç”¨çš„GPUç¼–å·')
    parser.add_argument('--turns', type=int, default=50, help='å¯¹è¯è½®æ•°')
    parser.add_argument('--num_traj', type=int, default=50, help='è½¨è¿¹æ•°é‡')
    args = parser.parse_args()
    
    # è®¾ç½®GPU
    os.environ["CUDA_VISIBLE_DEVICES"] = str(args.gpu)
    
    log("="*70)
    log("å¸¦æŒ‡æ ‡è¿½è¸ªçš„å¹¶å‘åŸºå‡†æµ‹è¯•")
    log("="*70)
    log(f"æ¨¡å‹: Qwen2.5-14B (128kä¸Šä¸‹æ–‡)")
    log(f"é…ç½®: GPU {args.gpu}, {args.num_traj}æ¡è½¨è¿¹, {args.turns}è½®å¯¹è¯")
    log(f"Batch Size: 2, 3, 4, 6, 8")
    log("="*70)
    
    # åŠ è½½
    log("\n[1/3] åŠ è½½é…ç½®å’Œæ•°æ®...")
    config = load_config()
    trajs = load_trajectories(args.num_traj)
    log(f"âœ“ åŠ è½½äº† {len(trajs)} æ¡è½¨è¿¹")
    log(f"  æ¯æ¡çº¦ {len(trajs[0].get('trajectory_steps', []))} æ­¥")
    
    # åŠ è½½æ¨¡å‹
    log(f"\n[2/3] åŠ è½½æ¨¡å‹åˆ°GPU {args.gpu}...")
    log("é…ç½®: ä¼˜åŒ–æ˜¾å­˜åˆ©ç”¨")
    start = time.time()
    runtime = sgl.Runtime(
        model_path=MODEL_PATH,
        tp_size=1,
        mem_fraction_static=0.88,  # æé«˜åˆ°88%ä»¥å……åˆ†åˆ©ç”¨æ˜¾å­˜
        max_total_tokens=196608    # 192k tokens (æé«˜50%ä»¥åˆ©ç”¨æ›´å¤šæ˜¾å­˜)
    )
    sgl.set_default_backend(runtime)
    load_time = time.time() - start
    log(f"âœ“ æ¨¡å‹åŠ è½½å®Œæˆ! {load_time:.1f}ç§’")
    
    log(f"\nGPU {args.gpu}æ˜¾å­˜ä½¿ç”¨:")
    os.system(f"nvidia-smi --query-gpu=index,memory.used,memory.total --format=csv,noheader,nounits | grep '^{args.gpu},'")
    
    # ä¼˜åŒ–è¯´æ˜
    log("\nğŸ’¡ ä¼˜åŒ–é…ç½®:")
    log("  â€¢ mem_fraction=0.88: æé«˜æ˜¾å­˜åˆ©ç”¨ç‡ (vs é»˜è®¤0.8)")
    log("  â€¢ max_total_tokens=192k: 50%æ›´å¤§çš„KV cache (vs 128k)")
    log("  â€¢ SGLangè‡ªåŠ¨prefix caching: å¤šè½®å¯¹è¯è‡ªåŠ¨å¤ç”¨å…±äº«å‰ç¼€")
    
    # æµ‹è¯•ä¸åŒbatch size
    log("\n[3/3] æµ‹è¯•ä¸åŒBatch Size (å¹¶å‘æ•°)...")
    
    num_turns = args.turns
    batch_sizes = [2, 3, 4, 6, 8]  # åªæµ‹è¿™5ä¸ª
    all_results = []
    
    # Metadataæ–‡ä»¶
    metadata_file = f"metrics_gpu{args.gpu}_{args.num_traj}traj_{num_turns}turns.jsonl"
    log(f"Metadataä¿å­˜åˆ°: {metadata_file}")
    
    # æ¸…ç©ºmetadataæ–‡ä»¶
    with open(metadata_file, 'w') as f:
        pass
    
    for batch_size in batch_sizes:
        try:
            log(f"\n{'>'*70}")
            log(f"å¼€å§‹æµ‹è¯• Batch Size = {batch_size}")
            result = benchmark_concurrent_with_metrics(
                runtime, trajs, batch_size, config, num_turns, metadata_file
            )
            all_results.append(result)
            time.sleep(2)
        except Exception as e:
            log(f"âœ— Batch Size {batch_size} å¤±è´¥: {str(e)[:150]}")
            import traceback
            traceback.print_exc()
            continue
    
    # æ±‡æ€»
    log("\n\n" + "="*70)
    log("æµ‹è¯•æ±‡æ€» - ä¸åŒBatch Sizeçš„å½±å“")
    log("="*70)
    print(f"{'å¹¶å‘':<6} {'æ€»è€—æ—¶':<12} {'è½¨è¿¹åå':<14} {'Tokenåå':<14} {'æ€»Tokens':<12} {'Tok/è½®':<10} {'Envæ—¶é—´':<10}")
    print(f"{'æ•°':<6} {'(ç§’)':<12} {'(æ¡/ç§’)':<14} {'(tok/ç§’)':<14} {'':<12} {'':<10} {'(ç§’)':<10}")
    print("-"*70)
    for r in all_results:
        print(f"{r['batch_size']:<6} "
              f"{r['total_time']:<12.2f} "
              f"{r['throughput_traj_per_sec']:<14.2f} "
              f"{r['throughput_token_per_sec']:<14.2f} "
              f"{r['total_tokens']:<12} "
              f"{r['avg_tokens_per_turn']:<10.1f} "
              f"{r['avg_env_time']:<10.2f}")
    log("="*70)
    
    # æ‰¾å‡ºæœ€ä¼˜
    if all_results:
        best_traj = max(all_results, key=lambda x: x['throughput_traj_per_sec'])
        best_token = max(all_results, key=lambda x: x['throughput_token_per_sec'])
        
        log(f"\nğŸ¯ æœ€ä¼˜é…ç½®:")
        log(f"  è½¨è¿¹ååæœ€é«˜: Batch Size={best_traj['batch_size']}, {best_traj['throughput_traj_per_sec']:.2f}æ¡/ç§’")
        log(f"  Tokenååæœ€é«˜: Batch Size={best_token['batch_size']}, {best_token['throughput_token_per_sec']:.2f}tok/ç§’")
    
    # ä¿å­˜æ±‡æ€»ç»“æœ
    output_file = f"summary_gpu{args.gpu}_{args.num_traj}traj_{num_turns}turns.json"
    with open(output_file, 'w') as f:
        json.dump({
            "model": "Qwen2.5-14B",
            "model_path": MODEL_PATH,
            "gpu": f"GPU {args.gpu}",
            "num_trajectories": args.num_traj,
            "num_turns": num_turns,
            "max_context": "128k tokens",
            "model_load_time": load_time,
            "timestamp": time.strftime('%Y-%m-%d %H:%M:%S'),
            "note": "Concurrent with detailed metrics tracking",
            "metadata_file": metadata_file,
            "results": all_results
        }, f, indent=2)
    log(f"\næ±‡æ€»ç»“æœå·²ä¿å­˜: {output_file}")
    log(f"è¯¦ç»†æŒ‡æ ‡å·²ä¿å­˜: {metadata_file}")
    log(f"  åŒ…å« {sum(r['time_series_points'] for r in all_results)} ä¸ªæ—¶é—´ç‚¹æ•°æ®")
    
    runtime.shutdown()
    log("\nâœ“ æ‰€æœ‰æµ‹è¯•å®Œæˆ!")

if __name__ == '__main__':
    main()

